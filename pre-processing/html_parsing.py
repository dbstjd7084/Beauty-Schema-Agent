import asyncio
import json
import os
from datetime import datetime
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from markdownify import markdownify as md

def refined_semantic_processor(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')

    # 1. ì œê±°í•  íƒœê·¸ (ë””ìì¸/ìŠ¤í¬ë¦½íŠ¸ ë…¸ì´ì¦ˆ)
    unwanted_tags = ['script', 'style', 'header', 'footer', 'nav', 'aside', 'noscript', 'meta', 'svg']
    for tag in soup(unwanted_tags):
        tag.decompose()

    img_count = 0
    vid_count = 0

    # 2. ëª¨ë“  íƒœê·¸ ìˆœíšŒ
    for tag in list(soup.find_all(True)):
        if not tag or not hasattr(tag, 'attrs'):
            continue
            
        # --- [ë¯¸ë””ì–´ ì‹ë³„ ì„¹ì…˜] ---
        if tag.name == 'img':
            img_url = tag.get('src') or tag.get('data-src') or "URL ì—†ìŒ"
            alt_text = tag.get('alt', 'ì„¤ëª… ì—†ìŒ').strip()
            tag.insert_before(f"\n\n[ğŸ–¼ï¸ ì´ë¯¸ì§€ ë°œê²¬ | ì„¤ëª…: {alt_text} | ê²½ë¡œ: {img_url}]\n\n")
            img_count += 1
            tag.decompose()
            continue
        
        elif tag.name == 'video':
            video_url = tag.get('src')
            if not video_url:
                source_tag = tag.find('source')
                video_url = source_tag.get('src') if source_tag else "URL ì—†ìŒ"
            tag.insert_before(f"\n\n[ğŸ¥ ë™ì˜ìƒ ë°œê²¬ | ê²½ë¡œ: {video_url}]\n\n")
            vid_count += 1
            tag.decompose()
            continue

        elif tag.name == 'iframe' and 'youtube' in (tag.get('src') or ''):
            yt_url = tag.get('src')
            tag.insert_before(f"\n\n[ğŸ“º ì™¸ë¶€ ë™ì˜ìƒ(YouTube) | ê²½ë¡œ: {yt_url}]\n\n")
            vid_count += 1
            tag.decompose()
            continue

        # --- [ì†ì„± í•„í„°ë§ ë° ê°€ì‹œí™” ì„¹ì…˜] ---
        new_attrs = {}
        current_attrs = tag.attrs if tag.attrs is not None else {}
        
        for attr, value in current_attrs.items():
            if attr == 'aria-label' or 'area' in attr or 'name' in attr:
                simplified_key = attr.replace('ap-click-', '')
                new_attrs[simplified_key] = value

        tag.attrs = new_attrs

        if new_attrs:
            attr_string = " | ".join([f"{k}: {v}" for k, v in new_attrs.items()])
            if tag.get_text(strip=True):
                tag.insert_before(f" {{#{attr_string}}} ")

    markdown_text = md(str(soup), heading_style="atx")
    print(f"âœ… ë¶„ì„ ì™„ë£Œ: ì´ë¯¸ì§€ {img_count}ê°œ, ë™ì˜ìƒ {vid_count}ê°œ ê°ì§€ë¨")
    
    # ë¯¸ë””ì–´ ê°œìˆ˜ ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜í•˜ê¸° ìœ„í•´ íŠœí”Œ í˜•íƒœë¡œ ë¦¬í„´
    return markdown_text.strip(), img_count, vid_count

async def run_crawler(target_url):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        print(f"ğŸŒ ì ‘ì† ì¤‘: {target_url}")
        
        try:
            await page.goto(target_url, wait_until="domcontentloaded", timeout=60000)
            await page.wait_for_timeout(7000) 
            await page.mouse.wheel(0, 2000)
            await page.wait_for_timeout(2000)

            content = await page.content()
            final_md, img_c, vid_c = refined_semantic_processor(content)
            
            # JSONì— ë‹´ì„ ë°ì´í„° êµ¬ì¡° ìƒì„±
            result_data = {
                "target_url": target_url,
                "crawl_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "media_stats": {
                    "image_count": img_c,
                    "video_count": vid_c
                },
                "info": final_md
            }
            
            return result_data

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
        finally:
            await browser.close()

if __name__ == "__main__":
    URL = "https://www.amoremall.com/kr/ko/aibc/web/"
    
    # 1. í¬ë¡¤ë§ ì‹¤í–‰ ë° ê²°ê³¼ ë°›ê¸° (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
    result_json = asyncio.run(run_crawler(URL))

    # 2. ê²°ê³¼ê°€ ìˆìœ¼ë©´ JSON íŒŒì¼ë¡œ ì €ì¥
    if result_json:
        file_name = "amore_live_output.json"
        
        # ì €ì¥ í´ë”ê°€ ì—†ë‹¤ë©´ ìƒì„± (ì„ íƒ ì‚¬í•­)
        # os.makedirs("output", exist_ok=True)
        # file_path = os.path.join("output", file_name)

        with open(file_name, "w", encoding="utf-8") as f:
            # indent=4ë¡œ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…, ensure_ascii=Falseë¡œ í•œê¸€ ë³´ì¡´
            json.dump(result_json, f, ensure_ascii=False, indent=4)
        
        print(f"\nğŸ“‚ JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_name}")
    else:
        print("\nâš ï¸ ì €ì¥í•  ê²°ê³¼ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")