import asyncio
import json
import os
from datetime import datetime
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from markdownify import markdownify as md

def refined_semantic_processor(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')

    # 1. ì œê±°í•  íƒœê·¸ (ë””ìì¸/ìŠ¤í¬ë¦½íŠ¸ ë…¸ì´ì¦ˆ)
    unwanted_tags = ['script', 'style', 'header', 'footer', 'nav', 'aside', 'noscript', 'meta', 'svg']
    for tag in soup(unwanted_tags):
        tag.decompose()

    img_count = 0
    vid_count = 0
    
    # [ì¶”ê°€] ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ Set (ì´ë¯¸ì§€ URLì˜ ê²½ë¡œ ê¸°ì¤€)
    seen_img_urls = set()
    # [ì¶”ê°€] í•„í„°ë§í•  í”Œë ˆì´ìŠ¤í™€ë” í‚¤ì›Œë“œ
    placeholders = ['750.png', 'defaultImages', 'blank.gif', 'loading.gif']

    # 2. ëª¨ë“  íƒœê·¸ ìˆœíšŒ
    for tag in list(soup.find_all(True)):
        if not tag or not hasattr(tag, 'attrs'):
            continue
            
        # --- [ë¯¸ë””ì–´ ì‹ë³„ ì„¹ì…˜] ---
        if tag.name == 'img':
            # [í•´ê²° 1] Lazy Loading ëŒ€ì‘: data-srcë¥¼ srcë³´ë‹¤ ìš°ì„ í•´ì„œ ê°€ì ¸ì˜´
            img_url = tag.get('data-src') or tag.get('src')
            
            # [í•´ê²° 2] Placeholder í•„í„°ë§: ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ëŠ” ìŠ¤í‚µ
            if not img_url or any(p in img_url for p in placeholders):
                tag.decompose()
                continue
            
            # [í•´ê²° 3] ì¤‘ë³µ ì œê±°: ì¿¼ë¦¬ ìŠ¤íŠ¸ë§(?...)ì„ ì œì™¸í•œ ìˆœìˆ˜ URLë¡œ ë¹„êµ
            base_url = img_url.split('?')[0]
            if base_url in seen_img_urls:
                tag.decompose()
                continue
            
            seen_img_urls.add(base_url)
            
            alt_text = tag.get('alt', 'ì„¤ëª… ì—†ìŒ').strip()
            # í…ìŠ¤íŠ¸ ë‚´ì— ì´ë¯¸ì§€ë¥¼ ì‚½ì…í•˜ëŠ” ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ìœ ì§€
            tag.insert_before(f"\n\n[ğŸ–¼ï¸ ì´ë¯¸ì§€ ë°œê²¬ | ì„¤ëª…: {alt_text} | ê²½ë¡œ: {img_url}]\n\n")
            img_count += 1
            tag.decompose()
            continue
        
        elif tag.name == 'video':
            video_url = tag.get('src')
            if not video_url:
                source_tag = tag.find('source')
                video_url = source_tag.get('src') if source_tag else "URL ì—†ìŒ"
            tag.insert_before(f"\n\n[ğŸ¥ ë™ì˜ìƒ ë°œê²¬ | ê²½ë¡œ: {video_url}]\n\n")
            vid_count += 1
            tag.decompose()
            continue

        elif tag.name == 'iframe' and 'youtube' in (tag.get('src') or ''):
            yt_url = tag.get('src')
            tag.insert_before(f"\n\n[ğŸ“º ì™¸ë¶€ ë™ì˜ìƒ(YouTube) | ê²½ë¡œ: {yt_url}]\n\n")
            vid_count += 1
            tag.decompose()
            continue

        # ë‚˜ë¨¸ì§€ ì†ì„± ì •ì œ ë¡œì§
        new_attrs = {}
        current_attrs = tag.attrs if tag.attrs is not None else {}
        
        for attr, value in current_attrs.items():
            if attr == 'aria-label' or 'area' in attr or 'name' in attr:
                simplified_key = attr.replace('ap-click-', '')
                new_attrs[simplified_key] = value

        tag.attrs = new_attrs
        if new_attrs:
            attr_string = " | ".join([f"{k}: {v}" for k, v in new_attrs.items()])
            if tag.get_text(strip=True):
                tag.insert_before(f" {{#{attr_string}}} ")

    markdown_text = md(str(soup), heading_style="atx")
    print(f"âœ… ë¶„ì„ ì™„ë£Œ: ì´ë¯¸ì§€ {img_count}ê°œ(ì¤‘ë³µ/í•„í„° ì œì™¸), ë™ì˜ìƒ {vid_count}ê°œ ê°ì§€ë¨")
    
    return markdown_text.strip(), img_count, vid_count

async def run_crawler(target_url):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        print(f"ğŸŒ ì ‘ì† ì¤‘: {target_url}")
        
        try:
            await page.goto(target_url, wait_until="networkidle", timeout=60000)
            
            for i in range(3):
                await page.mouse.wheel(0, 1500)
                await page.wait_for_timeout(1500) # ë¡œë”© ëŒ€ê¸°

            content = await page.content()
            final_md, img_c, vid_c = refined_semantic_processor(content)
            
            result_data = {
                "target_url": target_url,
                "crawl_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "media_stats": {
                    "image_count": img_c,
                    "video_count": vid_c
                },
                "info": final_md
            }
            
            return result_data

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
        finally:
            await browser.close()

if __name__ == "__main__":
    URL = "https://www.amoremall.com/kr/ko/store/gate?srsltid=AfmBOooPQQmPZ2Ky_nz7qhF_GOp4aag5oM7w3jrBwyP-rHgyCwe7TOiH"
    
    result_json = asyncio.run(run_crawler(URL))

    if result_json:
        file_name = "amore_refined_output.json"
        with open(file_name, "w", encoding="utf-8") as f:
            json.dump(result_json, f, ensure_ascii=False, indent=4)
        print(f"\nğŸ“‚ ì •ì œëœ JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_name}")
    else:
        print("\nâš ï¸ ì €ì¥í•  ê²°ê³¼ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")